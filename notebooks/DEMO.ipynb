{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.replace('?', dataSet.replace(['-'], [None]) # or .replace('-', {0: None})\n",
    "                \n",
    "# dataSet=dataSet.replace('?', 'None')\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "dataSet = dataSet.withColumn('WorkClass', regexp_replace('WorkClass', '?', 'ln'))\n",
    "                \n",
    "dataSet.createOrReplaceTempView('df')\n",
    "spark.sql('SELECT Age,WorkClass,Occupation FROM df where age=41 or age=72').show()\n",
    "                \n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Create a DataFrame\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, '?', 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, '?']))], [\"label\", \"features\"])\n",
    "                \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    " \n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    " \n",
    "employee1 = Employee('Basher', 'armbrust', 'bash@edureka.co', 100000)\n",
    "employee2 = Employee('Daniel', 'meng', 'daniel@stanford.edu', 120000 )\n",
    "employee3 = Employee('Muriel', None, 'muriel@waterloo.edu', 140000 )\n",
    "employee4 = Employee('Rachel', 'wendell', 'rach_3@edureka.co', 160000 )\n",
    "employee5 = Employee('Zach', 'galifianakis', 'zach_g@edureka.co', 160000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employee\n",
    "employee1\n",
    "employee2\n",
    "print(Employee[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department1 = Row(id='123456', name='HR')\n",
    "department2 = Row(id='789012', name='OPS')\n",
    "department3 = Row(id='345678', name='FN')\n",
    "department4 = Row(id='901234', name='DEV')\n",
    "\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2, employee5])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4, employee3])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departmentsWithEmployees_Seq = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "dframe = spark.createDataFrame(departmentsWithEmployees_Seq)\n",
    "display(dframe)\n",
    "dframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Column Names: ', dframe.columns) #Column Names\n",
    " \n",
    "print('Total Number of Rows = ', dframe.count() ) #Row Count\n",
    " \n",
    "print('Total Number of Columns = ', len(dframe.columns)) #Column Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe a Particular Column\n",
    "dframe.describe('department')\n",
    "dframe.describe('employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe.select('department').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesA = [('?',1),('Monkey',2),('?',3),('Spaghetti',4)]\n",
    "TableA = spark.createDataFrame(valuesA,['name','id'])\n",
    " \n",
    "valuesB = [('Rutabaga',1),('Pirate',2),('Ninja',3),('Darth Vader',4)]\n",
    "TableB = spark.createDataFrame(valuesB,['name','id'])\n",
    " \n",
    "TableA.show()\n",
    "TableB.show()\n",
    "\n",
    "TableA=TableA.na.replace('?', None)\n",
    "TableA.show()\n",
    "\n",
    "TableA=TableA.na.drop(how='any')\n",
    "TableA.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = TableA.alias('ta')\n",
    "tb = TableB.alias('tb')\n",
    "\n",
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join -- Default \n",
    "#inner_join = ta.join(tb, ta.name == tb.name)\n",
    "inner_join = ta.join(tb, ta.name == tb.name,'inner')\n",
    "inner_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark Left Outer Join\n",
    "\n",
    "left_join = ta.join(tb, ta.name == tb.name,how='left') # Could also use 'left_outer'\n",
    "\n",
    "left_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark Left Outer join with filter\n",
    "left_join = ta.join(tb, ta.name == tb.name,how='left') # Could also use 'left_outer'\n",
    "left_join.filter(col('tb.name').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark Right Outer Join\n",
    "right_join = ta.join(tb, ta.name == tb.name,how='right') # Could also use 'right_outer'\n",
    "right_join.show()\n",
    "right_join.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before: ', dataSet.count())\n",
    "dataSet=dataSet.na.replace('?', None)\n",
    "\n",
    "\n",
    "dataSet=dataSet.na.drop(how='any')\n",
    "print('After: ', dataSet.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet=dataSet.select('WorkClass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataSet=dataSet.replace('?',None).dropna(how='any')\n",
    "\n",
    "dataSet=dataSet.na.replace('?', None).dropna(how='any')\n",
    "\n",
    "\n",
    "targetDf = dataSet.withColumn(\"WorkClass\", \\\n",
    "              when(dataSet[\"WorkClass\"] == \"?\", None).otherwise(dataSet[\"WorkClass\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructing my DataFrame based on your assumptions\n",
    "# cols are Columns in the DataFrame\n",
    "cols = ['name', 'age', 'col_with_string']\n",
    "\n",
    "# Similarly the values\n",
    "vals = [\n",
    "     ('James', 18, 'passed'),\n",
    "     ('Smith', 15, 'passed'),\n",
    "     ('Albie', 32, 'failed'),\n",
    "     ('Stacy', 33, None),\n",
    "     ('Morgan', 11, None),\n",
    "     ('Dwight', 12, None),\n",
    "     ('Steve', 16, 'passed'), \n",
    "     ('Shroud', 22, 'passed'),\n",
    "     ('Faze', 11,'failed'),\n",
    "     ('Simple', 13, None)\n",
    "]\n",
    "\n",
    "# This will create a DataFrame using 'cols' and 'vals'\n",
    "# spark is an object of SparkSession\n",
    "df = spark.createDataFrame(vals, cols)\n",
    "# We have the following DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('col_with_string', when(df.col_with_string.isNull(), \n",
    "lit('0')).otherwise(df.col_with_string))\n",
    "\n",
    "# We have replaced nulls with a '0'\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.withColumn(\"can_vote\", col('Age') >= 18)\n",
    "#df = df.withColumn(\"can_lotto\", col('Age') > 16) \n",
    "\n",
    "#df = df.withColumn('col_with_string', when(df.col_with_string == '30',lit('?')).otherwise(df.col_with_string))\n",
    "\n",
    "df = df.withColumn('col_with_string', when(df.col_with_string == 'passed','?').otherwise(df.col_with_string))\n",
    "df.show()\n",
    "df = df.withColumn('col_with_string', when(df.col_with_string == '?',None).otherwise(df.col_with_string))\n",
    "df.show()\n",
    "#df = df.withColumn('col_with_string', when(df.col_with_string.equals('passed'),lit('0')).otherwise(df.col_with_string))\n",
    "\n",
    "df=df.dropna(how='any')\n",
    "# Updated DataFrame will be\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}